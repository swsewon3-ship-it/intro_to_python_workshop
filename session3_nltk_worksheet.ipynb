{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swsewon3-ship-it/intro_to_python_workshop/blob/main/session3_nltk_worksheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6603cae",
      "metadata": {
        "id": "f6603cae"
      },
      "source": [
        "\n",
        "# üìù Python for Public Policy ‚Äî Session 3 Worksheet  \n",
        "**Theme:** Text Analysis with NLTK (Inaugural Speeches or Your Own `.txt`)\n",
        "\n",
        "Use this notebook during the 60‚Äëminute student-driven block. **No presentations** at the end.  \n",
        "You may analyze an NLTK inaugural speech **or** upload your own plain text file (`.txt`).\n",
        "\n",
        "**Timing guide:**  \n",
        "- Part A: 10 min ¬∑ Part B: 10 min ¬∑ Part C: 10 min ¬∑ Part D: 10 min ¬∑ Part E: 10 min ¬∑ Part F: 10 min\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5a58467",
      "metadata": {
        "id": "f5a58467"
      },
      "source": [
        "\n",
        "## Part 0 ‚Äî Setup (run once)\n",
        "Upgrade NLTK (Colab often pins an older version) and download the datasets we need.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68206010",
      "metadata": {
        "id": "68206010"
      },
      "outputs": [],
      "source": [
        "\n",
        "# We upgrade NLTK to the latest version so functions and data work as expected.\n",
        "!pip -q install -U nltk\n",
        "\n",
        "# Now we import nltk and download only the small packages we need for this session.\n",
        "import nltk\n",
        "needed = [\"punkt_tab\",           # tokenizer (splits text into words)\n",
        "          \"stopwords\",       # list of common words to remove (the, and, is, ...)\n",
        "          \"wordnet\",         # lexical database used for lemmatization (getting word roots)\n",
        "          \"averaged_perceptron_tagger_eng\",  # part-of-speech tagger (N/V/Adj/Adv)\n",
        "          \"inaugural\"]       # a small corpus of U.S. inaugural addresses\n",
        "for pkg in needed:\n",
        "    nltk.download(pkg, quiet=True) # no status text printed - just downloads\n",
        "\n",
        "print(\"NLTK version:\", nltk.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b9e4e1d",
      "metadata": {
        "id": "5b9e4e1d"
      },
      "source": [
        "\n",
        "## Part A ‚Äî Pick Your Text (10 min)\n",
        "Choose **one** source below.\n",
        "\n",
        "### Option 1 ‚Äî Use the built-in Inaugural Speeches corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40d2ad51",
      "metadata": {
        "id": "40d2ad51"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.corpus import inaugural\n",
        "fileids = inaugural.fileids()\n",
        "fileids[:10]  # preview a few\n",
        "# Choose one speech (change the index or pick a specific filename from 'fileids')\n",
        "s1 = fileids[-5]\n",
        "raw = inaugural.raw(s1)\n",
        "print(\"Analyzing:\", s1, \"| characters:\", len(raw))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e8e1a3",
      "metadata": {
        "id": "71e8e1a3"
      },
      "source": [
        "\n",
        "### Option 2 ‚Äî Upload your own `.txt`\n",
        "> Must be **plain text** (e.g., speech, policy memo, report). If you upload multiple files, we‚Äôll read the first one.\n",
        "\n",
        "\n",
        "If you don‚Äôt already have a `.txt` file (like the inaugural corpus examples), you can **make one directly in Colab**.  \n",
        "This is helpful if you want to analyze text copied from a website, a PDF, or your own writing.\n",
        "\n",
        "üìù Example: paste any text (such as a news article, policy speech, or blog post) into the triple quotes below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ed2b83",
      "metadata": {
        "id": "02ed2b83"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a plain-text file in Colab\n",
        "text = \"\"\"Paste your speech or policy text here.\n",
        "You can copy/paste from a website or document.\n",
        "Keep it short at first (a few paragraphs).\"\"\"\n",
        "\n",
        "# Write the text into a .txt file\n",
        "with open(\"mytext.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "print(\"‚úÖ Saved mytext.txt\")\n",
        "\n",
        "# Load your saved .txt file\n",
        "with open(\"mytext.txt\", encoding=\"utf-8\") as f:\n",
        "    raw = f.read()\n",
        "\n",
        "print(raw[:500])  # print first 500 characters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60cb2b1",
      "metadata": {
        "id": "f60cb2b1"
      },
      "source": [
        "\n",
        "**Q:** Which text did you choose and why? *(Write a 1‚Äì2 sentence note below.)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b98b454a",
      "metadata": {
        "id": "b98b454a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Your short reflection:\n",
        "choice_note = \"\"\"\n",
        "(Explain your choice here.)\n",
        "\"\"\"\n",
        "print(choice_note)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fc2e3e5",
      "metadata": {
        "id": "7fc2e3e5"
      },
      "source": [
        "\n",
        "## Part B ‚Äî Tokenize & Explore (10 min)\n",
        "Lowercase and keep only alphabetic tokens (drop punctuation/numbers) for a quick clean view.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea7ba1f7",
      "metadata": {
        "id": "ea7ba1f7"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize: split into tokens, lowercase them, keep alphabetic tokens only\n",
        "tokens = [w.lower() for w in word_tokenize(raw) if w.isalpha()]\n",
        "len(tokens), tokens[:20]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "662866aa",
      "metadata": {
        "id": "662866aa"
      },
      "source": [
        "\n",
        "**Questions:**  \n",
        "1) How many tokens (words) are in your text?  \n",
        "2) List 5 tokens you expected to see. Were they present?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c014826",
      "metadata": {
        "id": "9c014826"
      },
      "source": [
        "\n",
        "## Part C ‚Äî Keyword in Context (10 min)\n",
        "Use *concordance* to see your keywords in context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21a6358a",
      "metadata": {
        "id": "21a6358a"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.text import Text\n",
        "T = Text(tokens)\n",
        "\n",
        "# Try two policy keywords of interest (edit these):\n",
        "T.concordance(\"economy\", width=70, lines=12)\n",
        "T.concordance(\"immigration\", width=70, lines=12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c80aaf",
      "metadata": {
        "id": "d5c80aaf"
      },
      "source": [
        "\n",
        "**Prompt:** In 2‚Äì3 sentences, what do the surrounding words suggest about how these topics are framed?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff99be15",
      "metadata": {
        "id": "ff99be15"
      },
      "outputs": [],
      "source": [
        "\n",
        "analysis_notes = \"\"\"\n",
        "(Write your brief interpretation here.)\n",
        "\"\"\"\n",
        "print(analysis_notes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5dfbf37",
      "metadata": {
        "id": "a5dfbf37"
      },
      "source": [
        "\n",
        "## Part D ‚Äî Dispersion Plot (10 min)\n",
        "Where do topics appear (beginning/middle/end)? Replace terms with 3‚Äì5 of your own.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d213dbeb",
      "metadata": {
        "id": "d213dbeb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# NLTK's dispersion_plot uses matplotlib under the hood\n",
        "T.dispersion_plot([\"economy\", \"war\", \"freedom\", \"security\", \"climate\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f37b8dff",
      "metadata": {
        "id": "f37b8dff"
      },
      "source": [
        "\n",
        "**Prompt:** What patterns do you notice? Why might this matter for policy framing?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "611a3062",
      "metadata": {
        "id": "611a3062"
      },
      "outputs": [],
      "source": [
        "\n",
        "framing_notes = \"\"\"\n",
        "(Write your observations here.)\n",
        "\"\"\"\n",
        "print(framing_notes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "215840fc",
      "metadata": {
        "id": "215840fc"
      },
      "source": [
        "\n",
        "## Part E ‚Äî Lexical Diversity (10 min)\n",
        "Lexical diversity = unique words / total words. Use a fixed slice for fair comparisons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ddddea",
      "metadata": {
        "id": "24ddddea"
      },
      "outputs": [],
      "source": [
        "\n",
        "def lexical_diversity(tok_list, n=None):\n",
        "    \"\"\"Return unique/total; if n is provided, compute on first n tokens.\"\"\"\n",
        "    if n:\n",
        "        tok_list = tok_list[:n]\n",
        "    total = len(tok_list) if tok_list else 1\n",
        "    return len(set(tok_list)) / total\n",
        "\n",
        "div_all = lexical_diversity(tokens)\n",
        "div_2k  = lexical_diversity(tokens, 2000)\n",
        "\n",
        "print(\"Lexical diversity (all tokens):\", round(div_all, 4))\n",
        "print(\"Lexical diversity (first 2,000):\", round(div_2k, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8e24c2e",
      "metadata": {
        "id": "b8e24c2e"
      },
      "source": [
        "\n",
        "**Why it matters (policy analysis):** Differences in lexical diversity can reflect changes in audience targeting, technicality vs. accessibility, or agenda breadth (e.g., a narrow crisis speech vs. a broad programmatic agenda).  \n",
        "\n",
        "*(Optional)* Compare with a second speech by repeating Part A for another `fileid` and recomputing diversity on the same slice size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc289ee1",
      "metadata": {
        "id": "cc289ee1"
      },
      "source": [
        "\n",
        "## Part F ‚Äî Reflection (10 min)\n",
        "- Which tool felt most useful for policy analysis (concordance, dispersion, lexical diversity)? Why?  \n",
        "- How could you apply this to a real policy dataset (hearings, legislation, news, social media)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef83bef",
      "metadata": {
        "id": "1ef83bef"
      },
      "outputs": [],
      "source": [
        "\n",
        "final_reflection = \"\"\"\n",
        "(Write your brief reflection here.)\n",
        "\"\"\"\n",
        "print(final_reflection)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a38ee4c",
      "metadata": {
        "id": "4a38ee4c"
      },
      "source": [
        "\n",
        "---\n",
        "## üöÄ Stretch Challenges (Optional)\n",
        "\n",
        "### 1) Collocations (common policy phrases)\n",
        "Find frequently co-occurring word pairs (bigrams). These can reveal informative policy phrases (e.g., *national security*, *climate change*).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccf4b77b",
      "metadata": {
        "id": "ccf4b77b"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
        "\n",
        "# Use a filtered token list (already lowercase & alphabetic)\n",
        "finder = BigramCollocationFinder.from_words(tokens)\n",
        "# Only consider reasonably frequent bigrams (adjust min_freq to taste)\n",
        "finder.apply_freq_filter(3)\n",
        "bigrams_pmi = finder.nbest(BigramAssocMeasures.pmi, 15)\n",
        "bigrams_pmi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcc38a52",
      "metadata": {
        "id": "bcc38a52"
      },
      "source": [
        "\n",
        "### 2) VADER sentiment on short snippets\n",
        "Useful for very short statements (tweets, headlines). For long speeches, sentiment averages toward neutral.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ee8fde",
      "metadata": {
        "id": "e0ee8fde"
      },
      "outputs": [],
      "source": [
        "\n",
        "# VADER is in nltk.sentiment\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "samples = [\n",
        "    \"We will rebuild our economy and create jobs.\",\n",
        "    \"War brings hardship and loss.\",\n",
        "    \"Together we can protect our freedoms.\"\n",
        "]\n",
        "\n",
        "for s in samples:\n",
        "    print(s, \"->\", sia.polarity_scores(s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3478974",
      "metadata": {
        "id": "f3478974"
      },
      "source": [
        "\n",
        "### 3) Compare two texts (quick)\n",
        "Repick another `fileid` and compute lexical diversity on the same slice (e.g., 2,000 tokens). What differences do you see?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6c18f2",
      "metadata": {
        "id": "5a6c18f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example scaffold:\n",
        "# s2 = fileids[-1]\n",
        "# raw2 = inaugural.raw(s2)\n",
        "# tokens2 = [w.lower() for w in word_tokenize(raw2) if w.isalpha()]\n",
        "# print(\"A:\", s1, \"div(2k) =\", round(lexical_diversity(tokens, 2000), 4))\n",
        "# print(\"B:\", s2, \"div(2k) =\", round(lexical_diversity(tokens2, 2000), 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b32b9bbe",
      "metadata": {
        "id": "b32b9bbe"
      },
      "source": [
        "\n",
        "### 4) Quick frequency table + matplotlib bar chart\n",
        "Show top 15 tokens (after simple cleaning). *Keep charts simple and readable.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d139120",
      "metadata": {
        "id": "1d139120"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fd = FreqDist(tokens)\n",
        "top = fd.most_common(15)\n",
        "\n",
        "labels, counts = zip(*top)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.bar(range(len(labels)), counts)\n",
        "plt.xticks(range(len(labels)), labels, rotation=45, ha='right')\n",
        "plt.title(\"Top 15 tokens\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}